{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Powered Ollama Server for LLMCache Experiments\n",
    "\n",
    "This notebook sets up Ollama on Kaggle's Tesla T4 GPU and exposes it via ngrok tunnel for remote access.\n",
    "\n",
    "**Usage:**\n",
    "1. Run all cells in order\n",
    "2. Copy the ngrok URL from the last cell\n",
    "3. Use this URL as `base_url` in your experiment config\n",
    "4. Keep this notebook running during experiments\n",
    "\n",
    "**Requirements:**\n",
    "- Enable GPU accelerator in Kaggle notebook settings\n",
    "- Have an ngrok account (free tier works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability and environment\nimport subprocess\nimport os\nimport sys\n\n# Suppress debugger warnings\nos.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n\ndef detect_environment():\n    \"\"\"Detect if running on Kaggle or locally\"\"\"\n    if os.path.exists('/kaggle'):\n        return 'kaggle'\n    elif 'COLAB_GPU' in os.environ:\n        return 'colab'\n    else:\n        return 'local'\n\nenv = detect_environment()\n\nprint(f\"üîç Environment: {env.upper()}\")\n\nif env == 'local':\n    print(\"‚ö†Ô∏è  WARNING: This notebook is designed for Kaggle GPU acceleration\")\n    print(\"   For local testing, use: docker-compose up\")\n    print(\"   To use Kaggle GPU:\")\n    print(\"   1. Upload this notebook to Kaggle\")\n    print(\"   2. Enable GPU accelerator in settings\")\n    print(\"   3. Run all cells\")\n    \ntry:\n    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)\n    if result.returncode == 0:\n        # Parse GPU info more cleanly\n        lines = result.stdout.split('\\n')\n        gpu_line = next((line for line in lines if 'Tesla' in line or 'GeForce' in line or 'RTX' in line), None)\n        if gpu_line:\n            gpu_name = gpu_line.split('|')[1].strip().split()[0:3]\n            print(f\"‚úÖ GPU detected: {' '.join(gpu_name)}\")\n        else:\n            print(\"‚úÖ GPU detected\")\n            \n        if env == 'kaggle':\n            print(\"üöÄ Perfect! Kaggle GPU environment ready\")\n    else:\n        print(\"‚ö†Ô∏è  nvidia-smi failed but GPU might still be available\")\n        \nexcept FileNotFoundError:\n    print(\"‚ùå No NVIDIA GPU detected (nvidia-smi not found)\")\n    if env != 'local':\n        print(\"   Make sure GPU accelerator is enabled in notebook settings\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  GPU check failed: {e}\")\n    \nprint(f\"üìç Ready to proceed with {env} setup\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install Ollama\nimport subprocess\nimport sys\n\nprint(\"üì¶ Installing Ollama...\")\n\ntry:\n    # Download and install Ollama with suppressed output\n    result = subprocess.run([\n        'bash', '-c', \n        'curl -fsSL https://ollama.com/install.sh | sh > /dev/null 2>&1'\n    ], capture_output=True, text=True, timeout=120)\n    \n    if result.returncode == 0:\n        print(\"‚úÖ Ollama installed successfully\")\n        \n        # Verify installation\n        verify = subprocess.run(['which', 'ollama'], capture_output=True, text=True)\n        if verify.returncode == 0:\n            print(f\"üìç Ollama location: {verify.stdout.strip()}\")\n        else:\n            print(\"‚ö†Ô∏è  Installation completed but ollama not found in PATH\")\n    else:\n        print(\"‚ùå Ollama installation failed\")\n        if result.stderr:\n            print(f\"Error: {result.stderr}\")\n            \nexcept subprocess.TimeoutExpired:\n    print(\"‚è±Ô∏è  Installation taking longer than expected...\")\n    print(\"This is normal for first-time Ollama installation\")\nexcept Exception as e:\n    print(f\"‚ùå Installation error: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyngrok for tunneling\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start Ollama service in background\nimport subprocess\nimport time\nimport os\nimport requests\n\n# Set environment variables for GPU usage\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n\nprint(\"üöÄ Starting Ollama server...\")\n\n# Start Ollama server\nollama_process = subprocess.Popen(['ollama', 'serve'], \n                                  stdout=subprocess.PIPE, \n                                  stderr=subprocess.PIPE,\n                                  text=True)\n\n# Wait and verify server startup\nmax_attempts = 10\nfor attempt in range(max_attempts):\n    time.sleep(2)\n    try:\n        # Test if server is responding\n        response = requests.get('http://localhost:11434/api/tags', timeout=5)\n        if response.status_code == 200:\n            print(\"‚úÖ Ollama server started successfully\")\n            print(f\"üì° Server running on port 11434\")\n            break\n    except requests.exceptions.RequestException:\n        if attempt == max_attempts - 1:\n            print(\"‚ùå Ollama server failed to start properly\")\n            print(\"Checking server output...\")\n            # Get any error output\n            try:\n                stdout, stderr = ollama_process.communicate(timeout=1)\n                if stderr:\n                    print(f\"Server error: {stderr[:200]}...\")\n            except:\n                pass\n        else:\n            print(f\"‚è≥ Waiting for server startup... ({attempt + 1}/{max_attempts})\")\n\nprint(\"üìç Server setup completed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pull the required model (adjust model name as needed)\nimport subprocess\nimport time\n\nMODEL_NAME = \"gemma2:9b\"  # Change this if needed\n\nprint(f\"üì• Downloading {MODEL_NAME} model...\")\nprint(\"‚è≥ This may take several minutes for large models\")\n\nstart_time = time.time()\n\ntry:\n    # Use ollama pull with minimal output\n    result = subprocess.run(['ollama', 'pull', MODEL_NAME], \n                          capture_output=True, text=True, timeout=600)  # 10 min timeout\n    \n    elapsed = time.time() - start_time\n    \n    if result.returncode == 0:\n        print(f\"‚úÖ Model {MODEL_NAME} downloaded successfully in {elapsed:.1f}s\")\n        \n        # Verify model is available\n        list_result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)\n        if MODEL_NAME in list_result.stdout:\n            # Parse model size from output\n            lines = list_result.stdout.split('\\n')\n            model_line = next((line for line in lines if MODEL_NAME in line), None)\n            if model_line:\n                parts = model_line.split()\n                if len(parts) >= 2:\n                    size = parts[1]\n                    print(f\"üìä Model size: {size}\")\n        \n    else:\n        print(f\"‚ùå Model download failed after {elapsed:.1f}s\")\n        # Show only first few lines of error to avoid spam\n        if result.stderr:\n            error_lines = result.stderr.split('\\n')[:3]\n            print(\"Error details:\")\n            for line in error_lines:\n                if line.strip():\n                    print(f\"  {line}\")\n                    \nexcept subprocess.TimeoutExpired:\n    elapsed = time.time() - start_time\n    print(f\"‚è±Ô∏è  Download timeout after {elapsed:.1f}s\")\n    print(\"Large models may take longer - consider using a smaller model for testing\")\nexcept Exception as e:\n    elapsed = time.time() - start_time\n    print(f\"‚ùå Download error after {elapsed:.1f}s: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test local Ollama\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup ngrok tunnel (you need to get your auth token from ngrok.com)\nfrom pyngrok import ngrok\nimport os\n\n# Get ngrok token from environment variable or prompt user\nNGROK_AUTH_TOKEN = os.getenv('NGROK_AUTH_TOKEN')\n\nif not NGROK_AUTH_TOKEN:\n    print(\"üîë ngrok auth token not found in environment\")\n    print(\"üìã To set up ngrok:\")\n    print(\"   1. Get your token from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n    print(\"   2. Set environment variable: NGROK_AUTH_TOKEN=your_token_here\")\n    print(\"   3. Or paste token below (not recommended for shared notebooks)\")\n    print()\n    \n    # Allow manual token entry (with warning)\n    manual_token = input(\"Enter ngrok token (or press Enter to skip): \").strip()\n    if manual_token:\n        NGROK_AUTH_TOKEN = manual_token\n        print(\"‚ö†Ô∏è  WARNING: Token entered manually - avoid sharing this notebook!\")\n    else:\n        print(\"‚ùå Skipping ngrok setup - no token provided\")\n\nif NGROK_AUTH_TOKEN:\n    try:\n        ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n        \n        # Create tunnel to Ollama port\n        public_tunnel = ngrok.connect(11434)\n        \n        print(f\"\\nüöÄ Ollama is now publicly accessible!\")\n        print(f\"üì° Public URL: {public_tunnel.public_url}\")\n        print(f\"\\nüìù Update your experiment.yaml with:\")\n        print(f'  model:')\n        print(f'    base_url: \"{public_tunnel.public_url}\"')\n        print(f\"\\n‚ö†Ô∏è  Keep this notebook running during experiments!\")\n        \n        # Store tunnel info for later cells\n        globals()['public_tunnel'] = public_tunnel\n        \n    except Exception as e:\n        print(f\"‚ùå ngrok setup failed: {e}\")\n        print(\"Check your auth token and try again\")\nelse:\n    print(\"‚è≠Ô∏è  Ngrok setup skipped\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the public endpoint\nimport requests\nimport json\n\n# Check if tunnel was created successfully\nif 'public_tunnel' not in globals():\n    print(\"‚ùå No ngrok tunnel available - run the previous cell first\")\nelse:\n    try:\n        print(\"üß™ Testing public Ollama endpoint...\")\n        \n        # Test with a simple generation request\n        response = requests.post(\n            f\"{public_tunnel.public_url}/api/generate\",\n            json={\n                \"model\": MODEL_NAME,  # Use the model from cell 5\n                \"prompt\": \"What is 2+2?\",\n                \"stream\": False\n            },\n            timeout=30\n        )\n        \n        if response.status_code == 200:\n            result = response.json()\n            test_response = result.get('response', 'No response field')\n            print(\"‚úÖ Ollama is working correctly!\")\n            print(f\"ü§ñ Test response: {test_response[:100]}...\")\n            \n            # Show model info\n            if 'model' in result:\n                print(f\"üìä Model used: {result['model']}\")\n                \n        else:\n            print(f\"‚ùå HTTP Error: {response.status_code}\")\n            print(f\"Response: {response.text[:200]}...\")\n            \n    except requests.exceptions.Timeout:\n        print(\"‚è±Ô∏è  Request timeout - model might be loading\")\n        print(\"Large models can take 30+ seconds for first request\")\n    except requests.exceptions.ConnectionError:\n        print(\"‚ùå Connection failed - check if Ollama server is running\")\n    except Exception as e:\n        print(f\"‚ùå Test failed: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Keep-alive cell - run this to prevent the notebook from timing out\nimport time\nimport datetime\nimport requests\n\nif 'public_tunnel' not in globals():\n    print(\"‚ùå No ngrok tunnel available - run the ngrok setup cell first\")\nelse:\n    print(\"üîÑ Keep-alive loop started. Ollama server is running...\")\n    print(f\"üåê Public URL: {public_tunnel.public_url}\")\n    print(\"\\n‚ö° Your GPU-powered Ollama is ready for experiments!\")\n    print(\"üìã To use: Update base_url in your experiment.yaml and run experiments\")\n    print(\"\\nüõë Press interrupt button or Ctrl+C to stop\")\n    \n    # Keep the server alive\n    try:\n        counter = 0\n        while True:\n            counter += 1\n            current_time = datetime.datetime.now().strftime(\"%H:%M:%S\")\n            \n            # Ping Ollama every 5 minutes to keep it alive\n            if counter % 30 == 0:  # Every 30 * 10s = 5 minutes\n                try:\n                    response = requests.get(f\"http://localhost:11434/api/tags\", timeout=5)\n                    status = \"üü¢ Online\" if response.status_code == 200 else \"üü° Warning\"\n                except:\n                    status = \"üî¥ Offline\"\n                \n                print(f\"[{current_time}] Ollama: {status} | Keep-alive #{counter//30}\")\n            elif counter % 6 == 0:  # Every minute show a dot\n                print(\".\", end=\"\", flush=True)\n            \n            time.sleep(10)  # Wait 10 seconds between checks\n            \n    except KeyboardInterrupt:\n        print(\"\\nüõë Keep-alive stopped. Shutting down...\")\n        try:\n            ngrok.disconnect(public_tunnel.public_url)\n            print(\"‚úÖ Tunnel closed.\")\n        except:\n            print(\"‚ö†Ô∏è  Tunnel cleanup failed\")\n    except Exception as e:\n        print(f\"\\n‚ùå Keep-alive error: {e}\")\n        print(\"Restarting keep-alive loop...\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}