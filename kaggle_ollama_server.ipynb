{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Powered Ollama Server for LLMCache Experiments\n",
    "\n",
    "This notebook sets up Ollama on Kaggle's Tesla T4 GPU and exposes it via ngrok tunnel for remote access.\n",
    "\n",
    "**Usage:**\n",
    "1. Run all cells in order\n",
    "2. Copy the ngrok URL from the last cell\n",
    "3. Use this URL as `base_url` in your experiment config\n",
    "4. Keep this notebook running during experiments\n",
    "\n",
    "**Requirements:**\n",
    "- Enable GPU accelerator in Kaggle notebook settings\n",
    "- Have an ngrok account (free tier works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyngrok for tunneling\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Ollama service in background\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set environment variables for GPU usage\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "\n",
    "# Start Ollama server\n",
    "print(\"Starting Ollama server...\")\n",
    "ollama_process = subprocess.Popen(['ollama', 'serve'], \n",
    "                                  stdout=subprocess.PIPE, \n",
    "                                  stderr=subprocess.PIPE,\n",
    "                                  text=True)\n",
    "\n",
    "# Wait for server to start\n",
    "time.sleep(10)\n",
    "print(\"Ollama server should be running on port 11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the required model (adjust model name as needed)\n",
    "!ollama pull gemma3:4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test local Ollama\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ngrok tunnel (you need to get your auth token from ngrok.com)\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Set your ngrok auth token here\n",
    "NGROK_AUTH_TOKEN = \"31uM6RvifhNp6hKjTvAgLv54eFF_7GCa84EDxxnHXyzD5tbHX\"  # Get from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "\n",
    "if NGROK_AUTH_TOKEN != \"your_ngrok_token_here\":\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    \n",
    "    # Create tunnel to Ollama port\n",
    "    public_tunnel = ngrok.connect(11434)\n",
    "    \n",
    "    print(f\"\\nüöÄ Ollama is now publicly accessible at:\")\n",
    "    print(f\"üì° {public_tunnel.public_url}\")\n",
    "    print(f\"\\nUpdate your experiment.yaml with:\")\n",
    "    print(f\"  base_url: \\\"{public_tunnel.public_url}\\\"\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Keep this notebook running during experiments!\")\n",
    "else:\n",
    "    print(\"‚ùå Please set your ngrok auth token above\")\n",
    "    print(\"Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the public endpoint\n",
    "import requests\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Test with a simple generation request\n",
    "    response = requests.post(\n",
    "        f\"{public_tunnel.public_url}/api/generate\",\n",
    "        json={\n",
    "            \"model\": \"gemma3:4b\",\n",
    "            \"prompt\": \"What is 2+2?\",\n",
    "            \"stream\": False\n",
    "        },\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\"‚úÖ Ollama is working correctly!\")\n",
    "        print(f\"Test response: {result.get('response', 'No response field')[:100]}...\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code} - {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep-alive cell - run this to prevent the notebook from timing out\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "print(\"üîÑ Keep-alive loop started. Ollama server is running...\")\n",
    "print(f\"üåê Public URL: {public_tunnel.public_url}\")\n",
    "print(\"\\n‚ö° Your GPU-powered Ollama is ready for experiments!\")\n",
    "print(\"üìã To use: Update base_url in your experiment.yaml and run experiments\")\n",
    "\n",
    "# Keep the server alive\n",
    "try:\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Ping Ollama every 5 minutes to keep it alive\n",
    "        if counter % 30 == 0:  # Every 30 * 10s = 5 minutes\n",
    "            try:\n",
    "                response = requests.get(f\"http://localhost:11434/api/tags\", timeout=5)\n",
    "                status = \"üü¢ Online\" if response.status_code == 200 else \"üü° Warning\"\n",
    "            except:\n",
    "                status = \"üî¥ Offline\"\n",
    "            \n",
    "            print(f\"[{current_time}] Ollama status: {status} | Keep-alive #{counter}\")\n",
    "        \n",
    "        time.sleep(10)  # Wait 10 seconds between checks\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Keep-alive stopped. Shutting down...\")\n",
    "    ngrok.disconnect(public_tunnel.public_url)\n",
    "    print(\"‚úÖ Tunnel closed.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
